{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "69bc8786-3ea7-4b21-a53f-09675d86534b",
      "metadata": {
        "collapsed": false,
        "name": "cell1"
      },
      "source": [
        "# pandas on Snowflake 101\n",
        "\n",
        "[pandas on Snowflake](https://docs.snowflake.com/developer-guide/snowpark/python/snowpark-pandas) gives Python developers the flexibility and convenience of pandas together with the power of Snowflake via a simple, unified, and familiar interface. Benefits of using pandas on Snowflake includes: \n",
        "\n",
        "- **Connected**: Easily work with Snowflake data, bring in data from files, and save back results\n",
        "- **Robust**: Develop pandas pipeline at all data scales from prototype to production\n",
        "- **Flexible**: Unlock powerful Snowflake analytics with familiar, flexible pandas API\n",
        "\n",
        "In this quickstart, we'll show how you can get started with using pandas on Snowflake. We'll also see that the Snowpark pandas API is very similar to the native pandas API and enables you to scale up your traditional pandas pipelines with just a few lines of change. You can run this notebook in a Snowflake Notebook. \n",
        "\n",
        "## Import Required Packages\n",
        "\n",
        "The Snowpark pandas API is available as part of the Snowpark Python package. Snowpark Python comes pre-installed with the Snowflake Notebooks environment. Additionally, you will need to add the `modin` package in the `Packages` dropdown.\n",
        "\n",
        "- To install Modin, select `modin` from `Packages` and ensure the version is 0.32.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b806a16b-b666-4e38-b11c-5db618772a12",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell2"
      },
      "outputs": [],
      "source": [
        "# Import the Snowpark pandas plugin for modin\n",
        "import snowflake.snowpark.modin.plugin\n",
        "import modin.pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e7b7455-c1ad-4ad8-bf85-1ed2b0d516b8",
      "metadata": {
        "collapsed": false,
        "name": "cell3"
      },
      "source": [
        "## Connecting to Snowflake \n",
        "\n",
        "To work with your data in Snowflake, you need to first get a session variable to connect to Snowflake. Since you are already logged in to Snowflake Notebook, you can get your session variable directly through the active notebook session. The session variable is the entrypoint that gives you access to using Python in Snowflake including pandas on Snowflake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea2342c8-f661-4f86-8245-813f8b7ad0ab",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell4"
      },
      "outputs": [],
      "source": [
        "# Access current Snowpark session\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "session = get_active_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39c2ccf7-2bbf-4726-99b5-4fb2e16a3405",
      "metadata": {
        "collapsed": false,
        "name": "cell5"
      },
      "source": [
        "## Generate Data Tables\n",
        "First let's generate synthetic data. Note that this will take about a minute but only needs to be run once. \n",
        "```sql\n",
        "CREATE OR REPLACE TABLE REVENUE_TRANSACTIONS_50M (Transaction_ID TEXT, Date DATE, Revenue FLOAT) AS\n",
        "SELECT\n",
        "  UUID_STRING() AS Transaction_ID,\n",
        "  DATEADD(DAY,UNIFORM(0, 10000, RANDOM()),'1998-01-01') AS Date,\n",
        "  UNIFORM(10, 1000, RANDOM()) * UNIFORM(10, 1000, RANDOM()) AS Revenue\n",
        "FROM\n",
        "  TABLE(GENERATOR(ROWCOUNT => 50000000));\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c7e9192-c1c5-40a9-b410-731746d6cd96",
      "metadata": {
        "collapsed": false,
        "name": "cell6"
      },
      "source": [
        "## Reading Data From Snowflake\n",
        "\n",
        "\n",
        "### ðŸŒ The Naive approach: Load data into in-memory pandas\n",
        "\n",
        "There are two common approaches to reading the data to vanilla pandas. However, both of these can be inefficient on large datasets.\n",
        "\n",
        "1) Create a [Snowpark DataFrame](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes#return-the-contents-of-a-dataframe-as-a-pandas-dataframe) and calling [`to_pandas`](https://docs.snowflake.com/developer-guide/snowpark/reference/python/latest/snowpark/api/snowflake.snowpark.DataFrame.to_pandas) to export results into a pandas DataFrame\n",
        "```python\n",
        "snowpark_df = session.table(\"REVENUE_TRANSACTIONS_50M\")\n",
        "native_pd_df = snowpark_df.to_pandas()\n",
        "```\n",
        "\n",
        "2) Use the [Snowflake Connector for Python](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-pandas) to query and export results from Snowflake into a pandas DataFrame using [`fetch_pandas_all`](https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api#fetch_pandas_all)\n",
        "\n",
        "```python\n",
        "# Create a cursor object\n",
        "cur = session.connection.cursor()\n",
        "# Execute a statement that will generate a result set\n",
        "cur.execute(\"select * from REVENUE_TRANSACTIONS_50M\")\n",
        "# Fetch all the rows in a cursor and load them into a pandas DataFrame\n",
        "native_pd_df = cur.fetch_pandas_all()\n",
        "```\n",
        "\n",
        "We will use the first approach below to demonstrate the time it takes to pull data into pandas in-memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae94052b-6caa-4d6b-ad3a-e51c3ce4b1c5",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell7"
      },
      "outputs": [],
      "source": [
        "from time import perf_counter\n",
        "start_time = perf_counter()\n",
        "table = session.table(\"REVENUE_TRANSACTIONS_50M\")\n",
        "pandas_df = table.to_pandas()\n",
        "end_time = perf_counter()\n",
        "time = end_time-start_time\n",
        "print(f\"Read to pandas dataframes takes {time} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72144502-fdd2-443f-aef1-cc23ff44fc5d",
      "metadata": {
        "collapsed": false,
        "name": "cell8"
      },
      "source": [
        "### ðŸš€ The Better Approach: `pd.read_snowflake`\n",
        "\n",
        "\n",
        "Now let's try this with pandas on Snowflake. We can read the table directly using Snowpark pandas's [`read_snowflake`](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/modin/pandas_api/modin.pandas.read_snowflake) command, which reads in the table by creating a reference to the underlying table, rather than pulling all the data into memory. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "235d2a5b-cfad-4143-baeb-246e0166dc52",
      "metadata": {
        "language": "python",
        "name": "cell9"
      },
      "outputs": [],
      "source": [
        "from time import perf_counter\n",
        "start_time = perf_counter()\n",
        "df = pd.read_snowflake(\"REVENUE_TRANSACTIONS_50M\")\n",
        "end_time = perf_counter()\n",
        "time = end_time-start_time\n",
        "print(f\"Read to Snowpark pandas dataframe takes {time} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8bf911a-4b93-436d-94e8-e49668de3c1b",
      "metadata": {
        "collapsed": false,
        "name": "cell10"
      },
      "source": [
        "As you can see, calling `read_snowflake` on any sized data takes no more than a few seconds  This scales even as we increase the row size to billions of rows, while this would almost lead to out of memory errors with in-memory pandas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "347b0783-f2c7-47cd-b3c3-a4933031b3b8",
      "metadata": {
        "collapsed": false,
        "name": "cell11"
      },
      "source": [
        "## The Power of `read_snowflake`\n",
        "`read_snowflake` doesn't only support reading in data from Snowflake tables, it also supports reading from Snowflake views, dynamic tables, iceberg tables, and more. Here you can see how read_snowflake can even take in a SQL query as an input and return a Snowpark pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "890fec2b-21e2-4987-91fc-3ff10b9e94e1",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell12"
      },
      "outputs": [],
      "source": [
        "summary_df = pd.read_snowflake(\"SELECT DATE_TRUNC ('MONTH', DATE) AS MONTH_DATE, SUM(REVENUE) AS TOTAL_REVENUE, COUNT(TRANSACTION_ID) AS TRANSACTION_COUNT FROM REVENUE_TRANSACTIONS_50M GROUP BY MONTH_DATE\")\n",
        "summary_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3ba323f-120b-4e8b-b53d-6426ab1591e3",
      "metadata": {
        "collapsed": false,
        "name": "cell13"
      },
      "source": [
        "You can even read from a view using `pd.read_snowflake`. Let's say that the SQL query we had earlier was used to define a view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5cd8083-a896-4b27-898d-418c2f800cb3",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "sql",
        "name": "cell14"
      },
      "outputs": [],
      "source": [
        "CREATE OR REPLACE VIEW SUMMARY_VIEW AS SELECT DATE_TRUNC ('MONTH', DATE) AS MONTH_DATE, SUM(REVENUE) AS TOTAL_REVENUE, COUNT(TRANSACTION_ID) AS TRANSACTION_COUNT FROM REVENUE_TRANSACTIONS_50M GROUP BY MONTH_DATE;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68b9b6f1-8636-4367-8b5f-bd886321fa60",
      "metadata": {
        "language": "python",
        "name": "cell15"
      },
      "outputs": [],
      "source": [
        "summary_df = pd.read_snowflake(\"SUMMARY_VIEW\")\n",
        "summary_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "326c7122-4954-4a92-8470-dc1a630b2e06",
      "metadata": {
        "collapsed": false,
        "name": "cell16"
      },
      "source": [
        "In summary,`pd.read_snowflake` is a convenient way for you to work with your Snowflake objects and intermix Python and SQL queries."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62c3dee5-62fe-46c6-a216-133a0140222d",
      "metadata": {
        "collapsed": false,
        "name": "cell17"
      },
      "source": [
        "## Examine and Profile Data\n",
        "Let's take a look at the data we're going to be working with. We will inspect the dataframe by printing out the first few rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a623bed-aed9-4cdb-a3c8-33e9e7da52af",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell18"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d377c362-3e67-420e-bdb6-81c2493c4456",
      "metadata": {
        "collapsed": false,
        "name": "cell19"
      },
      "source": [
        "We can look at the size and overall descriptive statistics of our dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1bb3625-2916-4e25-8308-eea77ecc3fbc",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell20"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01b11639-5329-4c2b-875b-9d28d462058b",
      "metadata": {
        "language": "python",
        "name": "cell21"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2822e6ae-9810-4ca1-8646-660eb3e68d97",
      "metadata": {
        "collapsed": false,
        "name": "cell22"
      },
      "source": [
        "## Data Transformations\n",
        "Let's take a look at some common data transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4218fceb-68f1-41be-8c08-3f6ad51424d5",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell23"
      },
      "outputs": [],
      "source": [
        "df[\"DATE\"] = pd.to_datetime(df[\"DATE\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f278b20f-e78d-4ab9-8d10-4c5427049f49",
      "metadata": {
        "collapsed": false,
        "name": "cell24"
      },
      "source": [
        "Filter to data only in the last 7 days based on the max date in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0278f8-e8f7-420e-ba7b-1782ef9e7366",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell25"
      },
      "outputs": [],
      "source": [
        "# Get the max date from the dataset\n",
        "max_date = df[\"DATE\"].max()\n",
        "# Filter for last 7 days from the max date\n",
        "filtered_df = df[(df[\"DATE\"] >= max_date - pd.Timedelta('7 days')) & (df[\"DATE\"] <= max_date)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d456c29-7689-4599-bcd6-02c646ef8f58",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell26"
      },
      "outputs": [],
      "source": [
        "print(f\"Before filtering, dataset size: {len(df)} rows. After filtering, dataset size: {len(filtered_df)} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d97817b5-d66e-4b1f-8d5b-5270b9d7abdc",
      "metadata": {
        "collapsed": false,
        "name": "cell27"
      },
      "source": [
        "The best part about this is that pandas on Snowflake automatically translates your pandas code into SQL and executed directly on Snowflake's engine, leading to significantly faster performance when working with large data. \n",
        "\n",
        "To show this in action, you can verify this by checking the `Query History` page to inspect the SQL query generated from your pandas operation.\n",
        "\n",
        "pandas on Snowflake supports a wide range of operationsâ€”like data cleaning, transformation, reshaping, using the familiar pandas API. You can see the list of currently supported APIs in Snowpark pandas [here](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/modin/supported/index).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7679bffe-696e-41b7-b41c-1ffffbde0f21",
      "metadata": {
        "collapsed": false,
        "name": "cell28"
      },
      "source": [
        "## Saving back to Snowflake\n",
        "\n",
        "Once you have developed your workflow, you can either save your results back to a table, view, files, dynamic table or iceberg table. We will show how you can save to a table and view in this demo. If you are interested in saving to a dynamic table to automatically refresh your pipeline as new data come in or saving it to a Iceberg table to leverage open table format, you can check out the example notebook [here](https://github.com/Snowflake-Labs/snowflake-python-recipes/blob/main/pandas%20pipeline%20with%20dynamic%20Iceberg%20tables/pandas%20pipeline%20with%20dynamic%20Iceberg%20tables.ipynb). \n",
        "\n",
        "You can use [to_snowflake](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/modin/pandas_api/modin.pandas.DataFrame.to_snowflake) to save your Snowpark pandas dataframe back to Snowflake as a table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a0c201b-4a79-4a1b-a564-6fad86802b8b",
      "metadata": {
        "language": "python",
        "name": "cell29"
      },
      "outputs": [],
      "source": [
        "filtered_df.to_snowflake(\"FILTERED_REVENUE_TRANSACTIONS\", if_exists=\"replace\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed25e269-cecc-42ee-93df-abe9360dda0a",
      "metadata": {
        "collapsed": false,
        "name": "cell30"
      },
      "source": [
        "To verify that the table has been created, we can run a simple SQL query to inspect the table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "864389b3-f059-4297-8366-81f589f7ad32",
      "metadata": {
        "language": "sql",
        "name": "cell31"
      },
      "outputs": [],
      "source": [
        "SELECT * FROM FILTERED_REVENUE_TRANSACTIONS LIMIT 5;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40096abe-83d8-4645-afd8-0213bf3c4e2e",
      "metadata": {
        "name": "cell32"
      },
      "source": [
        "You can also save your pandas workflow as a view. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa6c42b6-f35b-4511-b49b-ff80eacecb63",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "python",
        "name": "cell33"
      },
      "outputs": [],
      "source": [
        "filtered_df.to_view(\"FILTERED_REVENUE_VIEW\", index = None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e998df7a-941a-4052-be5b-97717731178c",
      "metadata": {
        "collapsed": false,
        "name": "cell34"
      },
      "source": [
        "Here you can see the view definition SQL statement that is generated by Snowpark pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cca0a285-c0a4-4de7-8ac0-d1dd85a988fb",
      "metadata": {
        "language": "sql",
        "name": "cell35"
      },
      "outputs": [],
      "source": [
        "SELECT GET_DDL('VIEW', 'FILTERED_REVENUE_VIEW');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06091134-c2c9-447a-ac6d-013b5b099bbd",
      "metadata": {
        "name": "cell36"
      },
      "source": [
        "This saves your Snowpark pandas operations as a pipeline that is then triggered when you access the view. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce59c1ea-278d-4029-81a4-6fbd706c2e0f",
      "metadata": {
        "language": "python",
        "name": "cell37"
      },
      "outputs": [],
      "source": [
        "# Read the view into a pandas DataFrame\n",
        "view_df = pd.read_snowflake(\"FILTERED_REVENUE_VIEW\")\n",
        "view_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b80dde4d-40ab-4821-b313-49772bf02471",
      "metadata": {
        "collapsed": false,
        "name": "cell38"
      },
      "source": [
        "## ðŸŽ Bonus: File read and write operations with pandas\n",
        "\n",
        "You can use pandas on Snowflake to load in [CSV](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/modin/pandas_api/modin.pandas.read_csv#modin.pandas.read_csv), [Parquet](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/modin/pandas_api/modin.pandas.read_parquet#modin.pandas.read_parquet), and [Excel](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/modin/pandas_api/modin.pandas.read_excel#modin.pandas.read_excel) from stage or local file location. Here is the full list of [I/O functionalities supported](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/modin/io).\n",
        "\n",
        "Here's how you can read CSV files from an S3 bucket\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f6010c8-cb7c-4589-912d-1251b4a8efaa",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false,
        "language": "sql",
        "name": "cell39"
      },
      "outputs": [],
      "source": [
        "-- First let's create a external stage and upload the CSV file. \n",
        "CREATE OR REPLACE STAGE FROSTBYTES\n",
        "    URL = 's3://sfquickstarts/frostbyte_tastybytes/';"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59da9b1c-a0a1-40c4-b983-74405898e608",
      "metadata": {
        "language": "python",
        "name": "cell40"
      },
      "outputs": [],
      "source": [
        "menu_df = pd.read_csv(\"@frostbytes/analytics/menu_item_aggregate_v.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bb14d3f",
      "metadata": {
        "name": "cell41"
      },
      "outputs": [],
      "source": [
        "menu_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c9bb36e-ec88-4ad6-8004-2968c548214d",
      "metadata": {
        "collapsed": false,
        "name": "cell42"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "In this quickstart, you saw how easy it is to get started with pandas on Snowflake. With minimal code changes, your existing pandas workflows can scale to larger datasets and run directly in Snowflakeâ€™s engine. \n",
        "pandas on Snowflake brings the flexibility and familiarity of the pandas API to the power and scale of Snowflake. It provides a simple, unified experience for Python developers to work efficiently with large datasetsâ€”all without moving data out of Snowflake.\n",
        "\n",
        "Key benefits include:\n",
        "\n",
        "- Connected â€“ Easily access Snowflake data, bring in files, and write results back\n",
        "- Robust â€“ Build pipelines that scale seamlessly from development to production\n",
        "- Flexible â€“ Use the familiar pandas API to unlock powerful Snowflake analytics\n",
        "\n",
        "To learn more, see [Snowflake Documentation](https://docs.snowflake.com/developer-guide/snowpark/python/snowpark-pandas). For a more advanced example, check out [this quickstart](https://quickstarts.snowflake.com/guide/data_engineering_pipelines_with_snowpark_pandas/) on how you can build a data engineering pipeline with Snowpark pandas."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
